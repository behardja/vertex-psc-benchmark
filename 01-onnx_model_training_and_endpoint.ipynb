{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UUTFwSh5u9OK"
   },
   "outputs": [],
   "source": [
    "# Copyright 2024 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qsLff0QbdE8D"
   },
   "source": [
    "# ONNX Serving on Vertex AI PSC Private Endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0C7kTlrH1bO8"
   },
   "source": [
    "## Overview\n",
    "\n",
    "Compared to the current PSA Private Endpoint, PSC based Private Endpoint has the following benefits:\n",
    "1. Simpler setup process: Currently, the only extra step user need to do is to create an Endpoint in their VPC. And this will be done by PSC automatically before our GA launch.\n",
    "\n",
    "2. No more IP exhuasted issue: GKE cluster will be hosted in tenant project VPC, so we can create much bigger cluster and won't affected by ip exhuasted issue in User's VPC.\n",
    "\n",
    "3. Unified experience with public endpoint: The API is the same as public endpoint, so user can use our SDK/client library. We also provide quota, IAM and monitoring metrics as public endpoint does.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RakMIliNYh8O"
   },
   "source": [
    "### Install Vertex AI SDK for Python and other required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nSHmJT9cTggu"
   },
   "outputs": [],
   "source": [
    "! pip3 install --upgrade --user --quiet google-cloud-aiplatform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! pip3 install tf2onnx \\\n",
    "               onnxruntime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "936Zz5YI2NeA"
   },
   "source": [
    "### Restart runtime (Colab only)\n",
    "\n",
    "To use the newly installed packages, you must restart the runtime on Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "66oJ55lG2Tiq"
   },
   "outputs": [],
   "source": [
    "# import sys\n",
    "\n",
    "# if \"google.colab\" in sys.modules:\n",
    "\n",
    "#     import IPython\n",
    "\n",
    "#     app = IPython.Application.instance()\n",
    "#     app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Clr61ben2WwY"
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>⚠️ The kernel is going to restart. Wait until it's finished before continuing to the next step. ⚠️</b>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v848aGbn2acH"
   },
   "source": [
    "### Authenticate your notebook environment (Colab only)\n",
    "\n",
    "Authenticate your environment on Google Colab.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IVeoyQPz2cfh"
   },
   "outputs": [],
   "source": [
    "# import sys\n",
    "\n",
    "# if \"google.colab\" in sys.modules:\n",
    "\n",
    "#     from google.colab import auth\n",
    "\n",
    "#     auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-01 19:58:46.490510: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tf2onnx\n",
    "import onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HId-ySlY2jlI"
   },
   "source": [
    "### Set Google Cloud project information and initialize Vertex AI SDK for Python\n",
    "\n",
    "To get started using Vertex AI, you must have an existing Google Cloud project and [enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com). Learn more about [setting up a project and a development environment](https://cloud.google.com/vertex-ai/docs/start/cloud-environment).\n",
    "\n",
    "- `PROJECT_ID`: Google Cloud project ID where Vertex AI resources are deployed\n",
    "- `LOCATION`: Google Cloud region where the Vertex AI endpoint is located\n",
    "- `BUCKET_URI`: Google Cloud Storage bucket URI to store model artifacts and other data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Y4gnZI9OX6VJ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = \"sandbox-401718\"  # @param {type:\"string\"}\n",
    "LOCATION = \"us-central1\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "moS794OKaaCt",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating gs://sandbox-401718-pred-benchmark/...\n",
      "ServiceException: 409 A Cloud Storage bucket named 'sandbox-401718-pred-benchmark' already exists. Try another name. Bucket names must be globally unique across all Google Cloud projects, including those outside of your organization.\n"
     ]
    }
   ],
   "source": [
    "# Create GCS Bucket\n",
    "BUCKET_URI = f\"gs://{PROJECT_ID}-pred-benchmark\"  # @param {type:\"string\"}\n",
    "! gsutil mb -l {LOCATION} -p {PROJECT_ID} {BUCKET_URI}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "-NrpFROTjoVL",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "aiplatform.init(project=PROJECT_ID, location=LOCATION, staging_bucket=BUCKET_URI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "swERjzZ-a_Nd",
    "tags": []
   },
   "source": [
    "## Prepare Test Models\n",
    "\n",
    "We prepared some test models, feel free to use your own models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.13.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"TensorFlow version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "# load daatset\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### TF Train Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2909 - accuracy: 0.9164\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1422 - accuracy: 0.9569\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1052 - accuracy: 0.9680\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0881 - accuracy: 0.9726\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0745 - accuracy: 0.9767\n",
      "313/313 - 1s - loss: 0.0762 - accuracy: 0.9775 - 576ms/epoch - 2ms/step\n",
      "INFO:tensorflow:Assets written to: saved_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_model/assets\n"
     ]
    }
   ],
   "source": [
    "# Build Model\n",
    "\n",
    "model = tf.keras.models.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "        tf.keras.layers.Dense(128, activation=\"relu\"),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(10),\n",
    "    ]\n",
    ")\n",
    "\n",
    "input_signature = [tf.TensorSpec(shape=(None, 28, 28), dtype=tf.float32, name=\"x\")]\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "model.compile(optimizer=\"adam\", loss=loss_fn, metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(x_train, y_train, epochs=5)\n",
    "model.evaluate(x_test, y_test, verbose=2)\n",
    "model.save(\"saved_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Load your own model\n",
    "# model = tf.keras.models.load_model(\"./saved_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -7.792286 ,  -2.4589105,  -6.757009 ,   7.791324 , -25.718307 ,\n",
       "         13.021902 , -12.477514 ,  -9.414201 , -12.708923 ,  -3.4792993]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tensorflow Model Predict\n",
    "predictions = model(x_train[:1]).numpy()\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference ONNX model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # # Load your own model\n",
    "# onnx_model = onnx.load(\"model.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-01 20:05:26.975973: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\n",
      "2025-04-01 20:05:26.976140: I tensorflow/core/grappler/clusters/single_machine.cc:357] Starting new session\n",
      "2025-04-01 20:05:27.009929: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\n",
      "2025-04-01 20:05:27.010128: I tensorflow/core/grappler/clusters/single_machine.cc:357] Starting new session\n"
     ]
    }
   ],
   "source": [
    "import tf2onnx\n",
    "import onnx\n",
    "\n",
    "onnx_model, _ = tf2onnx.convert.from_keras(model, input_signature, opset=13)\n",
    "onnx.save(onnx_model, \"./model.onnx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ -7.792286 ,  -2.4589112,  -6.7570105,   7.7913237, -25.718304 ,\n",
       "          13.021903 , -12.477514 ,  -9.414201 , -12.708925 ,  -3.4793   ]],\n",
       "       dtype=float32)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from onnxruntime import InferenceSession\n",
    "import numpy as np\n",
    "\n",
    "# Path to your ONNX model\n",
    "onnx_model_path = \"model.onnx\"\n",
    "\n",
    "input_data = x_train[:1].astype(np.float32)\n",
    "\n",
    "# Create an inference session\n",
    "session = InferenceSession(onnx_model_path)\n",
    "\n",
    "# Get the input name\n",
    "input_name = session.get_inputs()[0].name\n",
    "\n",
    "# Run inference.\n",
    "outputs = session.run(None, {input_name: input_data})\n",
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build custom Serving container\n",
    "\n",
    "- `ARTIFACT_REPO`: (Prerequisite) Name of the Artifact Registry repository to store the custom serving container image\n",
    "- `JOB_IMAGE_ID`: Name of the Docker image for the custom serving container\n",
    "- `VERSION`: Version or tag of the Docker image. Default set as latest.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ARTIFACT_REPO  = \"workbench\" # @param {type:\"string\"} ######################\n",
    "JOB_IMAGE_ID = \"vertex-custom-serve\" # @param {type:\"string\"}\n",
    "VERSION = \"latest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile app.py\n",
    "\n",
    "import os\n",
    "from typing import List\n",
    "import numpy as np\n",
    "from onnxruntime import InferenceSession\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel\n",
    "import asyncio\n",
    "\n",
    "# Input\n",
    "class PredictionInput(BaseModel):\n",
    "    instances: List[List[List[List[float]]]]\n",
    "\n",
    "# Variables\n",
    "MODEL_PATH = \"/app/model.onnx\"\n",
    "AIP_HEALTH_ROUTE = os.environ.get(\"AIP_HEALTH_ROUTE\", \"/health\")\n",
    "AIP_PREDICT_ROUTE = os.environ.get(\"AIP_PREDICT_ROUTE\", \"/predict\")\n",
    "\n",
    "# initiate serving server\n",
    "app = FastAPI(title=\"Serving Model\")\n",
    "\n",
    "# load model\n",
    "@app.on_event(\"startup\")\n",
    "async def load_inference_session():\n",
    "    global session\n",
    "    session = InferenceSession(MODEL_PATH)\n",
    "\n",
    "# check health\n",
    "@app.get(AIP_HEALTH_ROUTE, status_code=200)\n",
    "async def health():\n",
    "    if session is None:\n",
    "        return dict(status=\"unhealthy model not loaded\")\n",
    "    return dict(status=\"healthz\")\n",
    "\n",
    "\n",
    "# prediction endpoint \n",
    "@app.post(AIP_PREDICT_ROUTE)\n",
    "async def predict(input_data: PredictionInput):\n",
    "\n",
    "    global session\n",
    "\n",
    "    if session is None:\n",
    "        raise HTTPException(status_code=503, detail=\"Model not loaded yet\")\n",
    "\n",
    "    instances = input_data.instances\n",
    "    instances = np.array(instances).astype(np.float32)[0]\n",
    "\n",
    "    # Get the input name\n",
    "    input_name = session.get_inputs()[0].name\n",
    "\n",
    "    # Run inference.\n",
    "    outputs = session.run(None, {input_name: instances})\n",
    "    print(outputs)\n",
    "\n",
    "    return dict(predictions=outputs[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile Dockerfile\n",
    "\n",
    "FROM python:3.10-slim\n",
    "\n",
    "COPY ./requirements.txt /app/requirements.txt\n",
    "COPY ./model.onnx /app/model.onnx\n",
    "COPY ./app.py /app/app.py\n",
    "WORKDIR ./app\n",
    "\n",
    "RUN apt-get update && apt-get install gcc libffi-dev -y\n",
    "\n",
    "RUN pip install -r requirements.txt\n",
    "\n",
    "EXPOSE 8080\n",
    "\n",
    "CMD [\"uvicorn\", \"--host\", \"0.0.0.0\", \"--port\", \"8080\", \"app:app\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon  2.464MB\n",
      "Step 1/9 : FROM python:3.10-slim\n",
      " ---> 797a4d7093b1\n",
      "Step 2/9 : COPY ./requirements.txt /app/requirements.txt\n",
      " ---> Using cache\n",
      " ---> abd4edfdb378\n",
      "Step 3/9 : COPY ./model.onnx /app/model.onnx\n",
      " ---> Using cache\n",
      " ---> 12b217a893d0\n",
      "Step 4/9 : COPY ./app.py /app/app.py\n",
      " ---> Using cache\n",
      " ---> d38bcf2b2c4f\n",
      "Step 5/9 : WORKDIR ./app\n",
      " ---> Using cache\n",
      " ---> e9e9642e406f\n",
      "Step 6/9 : RUN apt-get update && apt-get install gcc libffi-dev -y\n",
      " ---> Using cache\n",
      " ---> 0616767449d1\n",
      "Step 7/9 : RUN pip install -r requirements.txt\n",
      " ---> Using cache\n",
      " ---> f3c04c21a414\n",
      "Step 8/9 : EXPOSE 8080\n",
      " ---> Using cache\n",
      " ---> df6d3db63751\n",
      "Step 9/9 : CMD [\"uvicorn\", \"--host\", \"0.0.0.0\", \"--port\", \"8080\", \"app:app\"]\n",
      " ---> Using cache\n",
      " ---> 4bc047f8d2b2\n",
      "Successfully built 4bc047f8d2b2\n",
      "Successfully tagged us-west2-docker.pkg.dev/sandbox-401718/workbench/vertex-custom-serve:latest\n",
      "\u001b[1;33mWARNING:\u001b[0m Your config file at [/home/jupyter/.docker/config.json] contains these credential helper entries:\n",
      "\n",
      "{\n",
      "  \"credHelpers\": {\n",
      "    \"gcr.io\": \"gcloud\",\n",
      "    \"us.gcr.io\": \"gcloud\",\n",
      "    \"eu.gcr.io\": \"gcloud\",\n",
      "    \"asia.gcr.io\": \"gcloud\",\n",
      "    \"staging-k8s.gcr.io\": \"gcloud\",\n",
      "    \"marketplace.gcr.io\": \"gcloud\",\n",
      "    \"us-west2-docker.pkg.dev\": \"gcloud\"\n",
      "  }\n",
      "}\n",
      "Adding credentials for: us-west2-docker.pkg.dev\n",
      "gcloud credential helpers already registered correctly.\n",
      "The push refers to repository [us-west2-docker.pkg.dev/sandbox-401718/workbench/vertex-custom-serve]\n",
      "\n",
      "\u001b[1Bb74b8417: Preparing \n",
      "\u001b[1Bac1d6134: Preparing \n",
      "\u001b[1B87b3214a: Preparing \n",
      "\u001b[1B071a1012: Preparing \n",
      "\u001b[1B913efc2d: Preparing \n",
      "\u001b[1B81090b57: Preparing \n",
      "\u001b[1B72ec4bd1: Preparing \n",
      "\u001b[1B86592bf9: Preparing \n",
      "\u001b[1B081d1eb2: Preparing \n",
      "\u001b[9Bac1d6134: Pushed   271.7MB/269.1MB\u001b[10A\u001b[2K\u001b[9A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[9A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[9A\u001b[2K\u001b[10A\u001b[2K\u001b[9A\u001b[2K\u001b[10A\u001b[2K\u001b[9A\u001b[2K\u001b[10A\u001b[2K\u001b[9A\u001b[2K\u001b[10A\u001b[2K\u001b[9A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[9A\u001b[2K\u001b[10A\u001b[2K\u001b[9A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[9A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[9A\u001b[2K\u001b[5A\u001b[2K\u001b[9A\u001b[2K\u001b[5A\u001b[2K\u001b[3A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[10A\u001b[2K\u001b[9A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[9A\u001b[2K\u001b[5A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[10A\u001b[2K\u001b[3A\u001b[2K\u001b[10A\u001b[2K\u001b[3A\u001b[2K\u001b[10A\u001b[2K\u001b[9A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[2A\u001b[2K\u001b[10A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[9A\u001b[2K\u001b[5A\u001b[2K\u001b[3A\u001b[2K\u001b[2A\u001b[2K\u001b[3A\u001b[2K\u001b[9A\u001b[2K\u001b[10A\u001b[2K\u001b[9A\u001b[2K\u001b[2A\u001b[2K\u001b[9A\u001b[2K\u001b[2A\u001b[2K\u001b[9A\u001b[2K\u001b[2A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[9A\u001b[2K\u001b[3A\u001b[2K\u001b[9A\u001b[2K\u001b[10A\u001b[2K\u001b[9A\u001b[2K\u001b[10A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[10A\u001b[2K\u001b[1A\u001b[2K\u001b[10A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[1A\u001b[2K\u001b[9A\u001b[2K\u001b[10A\u001b[2K\u001b[1A\u001b[2K\u001b[9A\u001b[2K\u001b[1A\u001b[2K\u001b[9A\u001b[2K\u001b[1A\u001b[2K\u001b[9A\u001b[2K\u001b[3A\u001b[2K\u001b[10A\u001b[2K\u001b[1A\u001b[2K\u001b[9A\u001b[2K\u001b[10A\u001b[2K\u001b[9A\u001b[2K\u001b[10A\u001b[2K\u001b[1A\u001b[2K\u001b[10A\u001b[2K\u001b[1A\u001b[2K\u001b[9A\u001b[2K\u001b[10A\u001b[2K\u001b[1A\u001b[2K\u001b[10A\u001b[2K\u001b[1A\u001b[2K\u001b[10A\u001b[2K\u001b[9A\u001b[2K\u001b[1A\u001b[2K\u001b[9A\u001b[2K\u001b[1A\u001b[2K\u001b[10A\u001b[2K\u001b[1A\u001b[2K\u001b[9A\u001b[2K\u001b[1A\u001b[2K\u001b[10A\u001b[2K\u001b[1A\u001b[2K\u001b[9A\u001b[2K\u001b[1A\u001b[2K\u001b[10A\u001b[2K\u001b[1A\u001b[2K\u001b[10A\u001b[2K\u001b[1A\u001b[2K\u001b[10A\u001b[2K\u001b[9A\u001b[2K\u001b[10A\u001b[2K\u001b[9A\u001b[2K\u001b[10A\u001b[2K\u001b[9A\u001b[2K\u001b[10A\u001b[2K\u001b[9A\u001b[2K\u001b[10A\u001b[2K\u001b[9A\u001b[2K\u001b[10A\u001b[2K\u001b[9A\u001b[2K\u001b[10A\u001b[2K\u001b[9A\u001b[2K\u001b[1A\u001b[2K\u001b[9A\u001b[2K\u001b[1A\u001b[2K\u001b[9A\u001b[2K\u001b[10A\u001b[2K\u001b[9A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[9A\u001b[2K\u001b[1A\u001b[2K\u001b[9A\u001b[2K\u001b[10A\u001b[2K\u001b[9A\u001b[2K\u001b[10A\u001b[2K\u001b[9A\u001b[2K\u001b[10A\u001b[2K\u001b[9A\u001b[2K\u001b[10A\u001b[2K\u001b[9A\u001b[2K\u001b[10A\u001b[2K\u001b[9A\u001b[2K\u001b[1A\u001b[2K\u001b[9A\u001b[2K\u001b[1A\u001b[2K\u001b[9A\u001b[2K\u001b[10A\u001b[2K\u001b[1A\u001b[2K\u001b[9A\u001b[2K\u001b[10A\u001b[2K\u001b[1A\u001b[2K\u001b[10A\u001b[2K\u001b[9A\u001b[2K\u001b[10A\u001b[2K\u001b[9A\u001b[2K\u001b[1A\u001b[2K\u001b[9A\u001b[2K\u001b[10A\u001b[2K\u001b[9A\u001b[2K\u001b[1A\u001b[2K\u001b[10A\u001b[2K\u001b[1A\u001b[2K\u001b[10A\u001b[2K\u001b[9A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[10A\u001b[2K\u001b[1A\u001b[2K\u001b[9A\u001b[2K\u001b[1A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[9A\u001b[2K\u001b[10A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[9A\u001b[2K\u001b[10A\u001b[2K\u001b[9A\u001b[2K\u001b[10A\u001b[2K\u001b[9A\u001b[2K\u001b[10A\u001b[2K\u001b[1A\u001b[2K\u001b[10A\u001b[2K\u001b[9A\u001b[2K\u001b[10A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[9A\u001b[2K\u001b[10A\u001b[2K\u001b[9A\u001b[2K\u001b[10A\u001b[2K\u001b[9A\u001b[2K\u001b[10A\u001b[2K\u001b[9A\u001b[2K\u001b[10A\u001b[2K\u001b[9A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[9A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[9A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[9A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[9A\u001b[2K\u001b[10A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[10A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[10A\u001b[2K\u001b[9A\u001b[2K\u001b[10A\u001b[2K\u001b[9A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[10A\u001b[2K\u001b[9A\u001b[2K\u001b[10A\u001b[2K\u001b[9A\u001b[2K\u001b[10A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[10A\u001b[2K\u001b[9A\u001b[2Klatest: digest: sha256:9b3ed696b3abf3680a23ed8d48d82e015464dee521d7e55e3f1b0e540a2ad7b8 size: 2420\n"
     ]
    }
   ],
   "source": [
    "# # Build and push image to reigstry\n",
    "! docker build . -f Dockerfile -t {LOCATION}-docker.pkg.dev/{PROJECT_ID}/{ARTIFACT_REPO}/{JOB_IMAGE_ID}:{VERSION}\n",
    "! gcloud auth configure-docker {LOCATION}-docker.pkg.dev --quiet\n",
    "! docker push {LOCATION}-docker.pkg.dev/{PROJECT_ID}/{ARTIFACT_REPO }/{JOB_IMAGE_ID}:{VERSION}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W7sbcii_iZ7x"
   },
   "source": [
    "### Upload Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "49Dak6icicSu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.models:Creating Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create Model backing LRO: projects/757654702990/locations/us-west2/models/2875552760122572800/operations/2943547108940054528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.models:Create Model backing LRO: projects/757654702990/locations/us-west2/models/2875552760122572800/operations/2943547108940054528\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model created. Resource name: projects/757654702990/locations/us-west2/models/2875552760122572800@1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.models:Model created. Resource name: projects/757654702990/locations/us-west2/models/2875552760122572800@1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To use this Model in another session:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.models:To use this Model in another session:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model = aiplatform.Model('projects/757654702990/locations/us-west2/models/2875552760122572800@1')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.models:model = aiplatform.Model('projects/757654702990/locations/us-west2/models/2875552760122572800@1')\n"
     ]
    }
   ],
   "source": [
    "model = aiplatform.Model.upload(\n",
    "    display_name=JOB_IMAGE_ID,\n",
    "    location = LOCATION,\n",
    "    serving_container_image_uri=f\"{LOCATION}-docker.pkg.dev/{PROJECT_ID}/{ARTIFACT_REPO }/{JOB_IMAGE_ID}:{VERSION}\",\n",
    "    serving_container_predict_route='/predict',\n",
    "    serving_container_health_route='/health',\n",
    "    serving_container_ports=[8080],\n",
    ")\n",
    "\n",
    "model.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JN2NkhEljbse"
   },
   "source": [
    "### Create PSC based Prediction Private Endpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "BqMtuRgPjqfD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PrivateEndpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.models:Creating PrivateEndpoint\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create PrivateEndpoint backing LRO: projects/757654702990/locations/us-west2/endpoints/6086804012391202816/operations/48014023517536256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.models:Create PrivateEndpoint backing LRO: projects/757654702990/locations/us-west2/endpoints/6086804012391202816/operations/48014023517536256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PrivateEndpoint created. Resource name: projects/757654702990/locations/us-west2/endpoints/6086804012391202816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.models:PrivateEndpoint created. Resource name: projects/757654702990/locations/us-west2/endpoints/6086804012391202816\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To use this PrivateEndpoint in another session:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.models:To use this PrivateEndpoint in another session:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "endpoint = aiplatform.PrivateEndpoint('projects/757654702990/locations/us-west2/endpoints/6086804012391202816')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.models:endpoint = aiplatform.PrivateEndpoint('projects/757654702990/locations/us-west2/endpoints/6086804012391202816')\n"
     ]
    }
   ],
   "source": [
    "psc_endpoint = aiplatform.PrivateEndpoint.create(\n",
    "    display_name=\"psc-endpoint\",\n",
    "    project=PROJECT_ID,\n",
    "    location=LOCATION,\n",
    "    private_service_connect_config=aiplatform.PrivateEndpoint.PrivateServiceConnectConfig(\n",
    "        project_allowlist=[PROJECT_ID],\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "USWCW-SNo-9M"
   },
   "source": [
    "### Deploy Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # # load existing model\n",
    "# model = aiplatform.Model(\"projects/757654702990/locations/us-central1/models/323587371566104576\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S_eRJglhpVfL"
   },
   "outputs": [],
   "source": [
    "psc_endpoint.deploy(model=model, traffic_percentage=100, machine_type=\"e2-standard-16\")\n",
    "\n",
    "psc_endpoint.list_models()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5uLFsbxpwzvN"
   },
   "source": [
    "### Create Forwarding Rule in Consumer Project\n",
    "\n",
    "- `NETWORK`: (Prerequisite) VPC network to use for the forwarding rule.\n",
    "- `subnet`: (Prerequisite) Subnet within the VPC network\n",
    "\n",
    "#### Best Practices\n",
    "Service attachment is a network resource that are used by multiple prediction endpoints. It is recommended to have a 1-1 mapping between the service attachment and forwarding rules/ip address. And this forwarding rule/ip address can be used to access all endpoints using the corresponding service attachment. \n",
    "Please note service attachment will only be preserved when there is active deployed model. If all models are undeployed from the endpoint for a while, the service attachment will be recycled and a new one will be created when there is a new model deployed. This means that the service attachment can change for the same endpoint if no active models are deployed. Then the forwarding rule should be deleted and recreated to with the new service attachment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load existing endpoint \n",
    "# psc_endpoint= aiplatform.PrivateEndpoint(endpoint_name=f\"projects/757654702990/locations/us-central1/endpoints/841510124806733824\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "EgjKUSAMnqvI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "projects/bde788be156874de5-tp/regions/us-west2/serviceAttachments/gkedpm-9b4fced59d3308dc0028e028117390\n"
     ]
    }
   ],
   "source": [
    "service_attachment = psc_endpoint.list_models()[0].private_endpoints.service_attachment\n",
    "print(service_attachment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6086804012391202816\n"
     ]
    }
   ],
   "source": [
    "endpoint_id = psc_endpoint.resource_name.rsplit('/', 1)[-1]\n",
    "print(endpoint_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using endpoint [https://us-west2-aiplatform.googleapis.com/]\n",
      "    serviceAttachment: projects/bde788be156874de5-tp/regions/us-west2/serviceAttachments/gkedpm-9b4fced59d3308dc0028e028117390\n"
     ]
    }
   ],
   "source": [
    "! gcloud ai endpoints describe {endpoint_id} \\\n",
    "--project={PROJECT_ID} \\\n",
    "--region={LOCATION} \\\n",
    "| grep -i serviceAttachment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5R2z2mUlMrl9"
   },
   "source": [
    "Then, create an address and a forwarding rule targeting at the service attachment. In this example, default network and subnet are used, replace it with your VPC network and subnet if running in your VPC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "NETWORK = 'beusebio-network' # @param {type:\"string\"}\n",
    "subnet = \"projects/sandbox-401718/regions/us-central1/subnetworks/beusebio-network\" # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created [https://www.googleapis.com/compute/v1/projects/sandbox-401718/regions/us-west2/addresses/psc-prediction].\n"
     ]
    }
   ],
   "source": [
    "! gcloud compute addresses create psc-prediction \\\n",
    "    --region={LOCATION} \\\n",
    "    --subnet={subnet}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created [https://www.googleapis.com/compute/v1/projects/sandbox-401718/regions/us-west2/forwardingRules/op-psc-endpoint].\n"
     ]
    }
   ],
   "source": [
    "! gcloud compute forwarding-rules create op-psc-endpoint \\\n",
    "    --network={NETWORK} \\\n",
    "    --address=psc-prediction \\\n",
    "    --target-service-attachment={service_attachment} \\\n",
    "    --region={LOCATION}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oL-74S0kVkym"
   },
   "source": [
    "Save the IP address above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bEtkfw1dTbvh"
   },
   "outputs": [],
   "source": [
    "IP_ADDRESS = ! gcloud compute forwarding-rules describe op-psc-endpoint --region={LOCATION} --format='value(IPAddress)'\n",
    "IP_ADDRESS = IP_ADDRESS[0]\n",
    "print(IP_ADDRESS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "civyNQaPr4QD"
   },
   "source": [
    "### Make Predictions\n",
    "\n",
    "Note: The endpoint is scoped to the region specified by the user, unless global accessibility is enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = {\"instances\": [[[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0117647061124444, 0.07058823853731155, 0.07058823853731155, 0.07058823853731155, 0.4941176474094391, 0.5333333611488342, 0.686274528503418, 0.10196078568696976, 0.6509804129600525, 1.0, 0.9686274528503418, 0.49803921580314636, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11764705926179886, 0.1411764770746231, 0.3686274588108063, 0.6039215922355652, 0.6666666865348816, 0.9921568632125854, 0.9921568632125854, 0.9921568632125854, 0.9921568632125854, 0.9921568632125854, 0.8823529481887817, 0.6745098233222961, 0.9921568632125854, 0.9490196108818054, 0.7647058963775635, 0.250980406999588, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1921568661928177, 0.9333333373069763, 0.9921568632125854, 0.9921568632125854, 0.9921568632125854, 0.9921568632125854, 0.9921568632125854, 0.9921568632125854, 0.9921568632125854, 0.9921568632125854, 0.9843137264251709, 0.364705890417099, 0.32156863808631897, 0.32156863808631897, 0.21960784494876862, 0.15294118225574493, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07058823853731155, 0.8588235378265381, 0.9921568632125854, 0.9921568632125854, 0.9921568632125854, 0.9921568632125854, 0.9921568632125854, 0.7764706015586853, 0.7137255072593689, 0.9686274528503418, 0.9450980424880981, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3137255012989044, 0.6117647290229797, 0.41960784792900085, 0.9921568632125854, 0.9921568632125854, 0.8039215803146362, 0.04313725605607033, 0.0, 0.16862745583057404, 0.6039215922355652, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.054901961237192154, 0.003921568859368563, 0.6039215922355652, 0.9921568632125854, 0.3529411852359772, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.545098066329956, 0.9921568632125854, 0.7450980544090271, 0.007843137718737125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04313725605607033, 0.7450980544090271, 0.9921568632125854, 0.27450981736183167, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13725490868091583, 0.9450980424880981, 0.8823529481887817, 0.6274510025978088, 0.42352941632270813, 0.003921568859368563, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3176470696926117, 0.9411764740943909, 0.9921568632125854, 0.9921568632125854, 0.46666666865348816, 0.09803921729326248, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1764705926179886, 0.729411780834198, 0.9921568632125854, 0.9921568632125854, 0.5882353186607361, 0.10588235408067703, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.062745101749897, 0.364705890417099, 0.9882352948188782, 0.9921568632125854, 0.7333333492279053, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9764705896377563, 0.9921568632125854, 0.9764705896377563, 0.250980406999588, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.18039216101169586, 0.5098039507865906, 0.7176470756530762, 0.9921568632125854, 0.9921568632125854, 0.8117647171020508, 0.007843137718737125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15294118225574493, 0.5803921818733215, 0.8980392217636108, 0.9921568632125854, 0.9921568632125854, 0.9921568632125854, 0.9803921580314636, 0.7137255072593689, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0941176488995552, 0.4470588266849518, 0.8666666746139526, 0.9921568632125854, 0.9921568632125854, 0.9921568632125854, 0.9921568632125854, 0.7882353067398071, 0.30588236451148987, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09019608050584793, 0.25882354378700256, 0.8352941274642944, 0.9921568632125854, 0.9921568632125854, 0.9921568632125854, 0.9921568632125854, 0.7764706015586853, 0.3176470696926117, 0.007843137718737125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07058823853731155, 0.6705882549285889, 0.8588235378265381, 0.9921568632125854, 0.9921568632125854, 0.9921568632125854, 0.9921568632125854, 0.7647058963775635, 0.3137255012989044, 0.03529411926865578, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.21568627655506134, 0.6745098233222961, 0.886274516582489, 0.9921568632125854, 0.9921568632125854, 0.9921568632125854, 0.9921568632125854, 0.95686274766922, 0.5215686559677124, 0.04313725605607033, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.5333333611488342, 0.9921568632125854, 0.9921568632125854, 0.9921568632125854, 0.8313725590705872, 0.529411792755127, 0.5176470875740051, 0.062745101749897, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]]]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-7.792285919189453, -2.458911180496216, -6.757010459899902, 7.791323661804199, -25.71830368041992, 13.0219030380249, -12.47751426696777, -9.414200782775879, -12.70892524719238, -3.479300022125244]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py:1103: InsecureRequestWarning: Unverified HTTPS request is being made to host '10.168.0.2'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "response = psc_endpoint.predict(\n",
    "            instances=data[\"instances\"], \n",
    "            endpoint_override=IP_ADDRESS\n",
    "        )\n",
    "print(response.predictions[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_TCktMxbA4mb",
    "tags": []
   },
   "source": [
    "### Deploy another model and update traffic split\n",
    "\n",
    "Deploy another model, and update the traffic split to be 50:50, after the deployment is done, you can rerun the prediction again for multiple times, you should be able to see the deployed_model_id are different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bVLgpRiRBEL7"
   },
   "outputs": [],
   "source": [
    "# psc_endpoint.deploy(model=model, traffic_percentage=50, machine_type=\"e2-standard-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2dx975IkCv7v"
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# if not os.getenv(\"IS_TESTING\"):\n",
    "#     import json\n",
    "\n",
    "#     import urllib3\n",
    "\n",
    "#     urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "#     counter = {}\n",
    "#     with open(REQUEST_FILE) as json_file:\n",
    "#         data = json.load(json_file)\n",
    "#         for i in range(1000):\n",
    "#             response = psc_endpoint.predict(\n",
    "#                 instances=data[\"instances\"], endpoint_override=IP_ADDRESS\n",
    "#             )\n",
    "#             if response.deployed_model_id in counter.keys():\n",
    "#                 counter[response.deployed_model_id] += 1\n",
    "#             else:\n",
    "#                 counter[response.deployed_model_id] = 1\n",
    "#     print(counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Hld7iDmEyiF"
   },
   "source": [
    "You can update the traffic split with the following command and run the code above again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HXPI-2q9Eh6X"
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# if not os.getenv(\"IS_TESTING\"):\n",
    "#     deployed_model_id_0 = list(counter)[0]\n",
    "#     deployed_model_id_1 = list(counter)[1]\n",
    "\n",
    "#     psc_endpoint.update(\n",
    "#         traffic_split={deployed_model_id_0: 20, deployed_model_id_1: 80}\n",
    "#     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XW_BtPnEFPp4"
   },
   "source": [
    "## Cleanup (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E4Ik3eKqdI_2"
   },
   "outputs": [],
   "source": [
    "psc_endpoint.undeploy_all()\n",
    "psc_endpoint.delete()\n",
    "model.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JRncavA6FSlc"
   },
   "outputs": [],
   "source": [
    "! gcloud compute forwarding-rules delete op-psc-endpoint --region={LOCATION}  --quiet\n",
    "\n",
    "! gcloud compute addresses delete psc-prediction --region={LOCATION} --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BTz-2N1XunXB"
   },
   "source": [
    "Delete the bucket if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NPQT5Wv9lC3O"
   },
   "outputs": [],
   "source": [
    "! gsutil rm -r {BUCKET_URI}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2iR_Q5K_ksWi"
   },
   "source": [
    "Optionally, you can use the following command to clean up all private endpoint and models if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vkyvlwTgky0J"
   },
   "outputs": [],
   "source": [
    "for pe in aiplatform.PrivateEndpoint.list():\n",
    "    pe.undeploy_all()\n",
    "    pe.delete()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "get_started_with_psc_private_endpoint.ipynb",
   "toc_visible": true
  },
  "environment": {
   "kernel": "conda-root-py",
   "name": "workbench-notebooks.m117",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m117"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
